{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agricultural Produce Classification Experiments\n",
    "\n",
    "This notebook contains experiments for the agricultural produce classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.model import ProduceClassifier\n",
    "from src.classify import ImageClassifier\n",
    "from utils.preprocessing import load_and_preprocess_images, extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images\n",
    "data_dir = '../data/train'\n",
    "produce_type = 'mango'\n",
    "\n",
    "# Load good and bad samples\n",
    "good_images = load_and_preprocess_images(f'{data_dir}/good', produce_type)\n",
    "bad_images = load_and_preprocess_images(f'{data_dir}/bad', produce_type)\n",
    "\n",
    "print(f'Good samples: {len(good_images)}')\n",
    "print(f'Bad samples: {len(bad_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Show good samples\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(good_images[i])\n",
    "    axes[0, i].set_title('Good')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Show bad samples\n",
    "for i in range(5):\n",
    "    axes[1, i].imshow(bad_images[i])\n",
    "    axes[1, i].set_title('Bad')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from images\n",
    "good_features = [extract_features(img) for img in good_images[:50]]\n",
    "bad_features = [extract_features(img) for img in bad_images[:50]]\n",
    "\n",
    "# Plot color distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "for idx, color in enumerate(colors):\n",
    "    good_values = [f[f'mean_{color}'] for f in good_features]\n",
    "    bad_values = [f[f'mean_{color}'] for f in bad_features]\n",
    "    \n",
    "    axes[idx].hist(good_values, alpha=0.5, label='Good', bins=20)\n",
    "    axes[idx].hist(bad_values, alpha=0.5, label='Bad', bins=20)\n",
    "    axes[idx].set_title(f'{color.upper()} Channel Distribution')\n",
    "    axes[idx].set_xlabel('Mean Value')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = '../models/mangoes.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_dir = '../data/test'\n",
    "test_good = load_and_preprocess_images(f'{test_dir}/good', produce_type)\n",
    "test_bad = load_and_preprocess_images(f'{test_dir}/bad', produce_type)\n",
    "\n",
    "# Combine test data\n",
    "X_test = np.concatenate([test_good, test_bad])\n",
    "y_test = np.concatenate([np.ones(len(test_good)), np.zeros(len(test_bad))])\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Bad', 'Good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "print(f'Number of misclassified samples: {len(misclassified_indices)}')\n",
    "\n",
    "# Visualize some misclassified samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(min(10, len(misclassified_indices))):\n",
    "    idx = misclassified_indices[i]\n",
    "    axes[i].imshow(X_test[idx])\n",
    "    axes[i].set_title(f'True: {\"Good\" if y_test[idx] else \"Bad\"}\\nPred: {\"Good\" if y_pred[idx] else \"Bad\"}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = model.predict(X_test).reshape(-1)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}